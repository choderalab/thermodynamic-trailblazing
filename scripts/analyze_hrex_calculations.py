#!/usr/env/python


"""

The script analyzes the netcdf files generated by the YANK simulations and
extracts/generate the data required for plots and statistics.

"""

import collections
import glob
import json
import math
import os

import numpy as np

from utils.analysis import get_analysis_cutoffs, get_free_energy_traj
from utils.protocol import read_experiment_protocol


YANK_DIR_PATH = os.path.join('..', 'yank')
EXPERIMENTS_DIR_PATH = os.path.join(YANK_DIR_PATH, 'experiments')
FREE_ENERGY_TRAJ_DIR_PATH = os.path.join(YANK_DIR_PATH, 'free_energy_trajectories')
THERMO_LEN_DIR_PATH = os.path.join(YANK_DIR_PATH, 'thermo_length')


# ============================================================
# FREE ENERGY TRAJECTORY ANALYSIS
# ============================================================

class FreeEnergyTrajectory:
    """A free energy trajectory.

    This make available the free energy trajectory data both as
    numpy arrays and as a dictionary.

    Attributes
    ----------
    data: Dict[int, Tuple[float]]
        data[n_energy_eval] is a tuple (free_energy, uncertainty) estimated
        at the given number of energy evaluations.
    energy_evals : numpy.ndarray
        The number of energy evaluations for each point in the trajectories.
    f_traj : numpy.ndarray
        The trajectory of the free energy differences.
    df_traj : numpy.ndarray
        The trajectory of the free energy uncertainties as computed by the
        MBAR class.
    f_boots : numpy.ndarray, optional
        The trajectory of the MBAR bootstrap samples (if there are any).

    """

    @classmethod
    def from_dict(cls, data):
        """Initialize the free energy trajectory from the dict representation.

        Parameters
        ----------
        data : Dict[int, Tuple[float]]
            data[n_energy_eval] is the tuple (free_energy, uncertainty[, bootstrap_fe)
            estimated at n_energy_eval.
        """
        energy_evals = np.array(sorted(data.keys()))
        f_traj = np.empty(len(energy_evals))
        df_traj = np.empty(len(energy_evals))

        # Check if there are bootstrap distributions.
        has_boots = len(data[energy_evals[0]]) > 2
        if has_boots:
            n_boots = len(data[energy_evals[0]][2])
            f_boots = np.empty(shape=(len(energy_evals), n_boots))

        # Convert dict to numpy arrays.
        for i, ee in enumerate(energy_evals):
            f_traj[i] = data[ee][0]
            df_traj[i] = data[ee][1]
            if has_boots:
                f_boots[i] = data[ee][2]

        # Initialize object.
        fet = cls()
        fet.data = data
        fet.energy_evals = energy_evals
        fet.f_traj = f_traj
        fet.df_traj = df_traj

        if has_boots:
            fet.f_boots = f_boots
        else:
            fet.f_boots = None

        return fet

    def __len__(self):
        return len(self.energy_evals)


def get_experiment_name(experiment_dir_path):
    """Return a readable name for the experiment in the given directory.

    Parameters
    ----------
    experiment_dir_path : str
        The path containing the nc files of the YANK experiment.

    Returns
    -------
    experiment_name : str
        A name for the experiment (e.g. "CB8-ligand1-trailblaze20-replicate2").

    """
    experiment_name = os.path.basename(os.path.normpath(experiment_dir_path))
    splitted_experiment_name = experiment_name.split('_')
    protocol_name = splitted_experiment_name[0]
    system_name = splitted_experiment_name[1]

    # Find the name of receptor and ligand.
    receptor_name = system_name[:system_name.find('system')]
    ligand_idx = int(system_name[system_name.find('ligands')+len('ligands'):])

    # Find the replicate number.
    if 'replicates' in experiment_dir_path:
        if len(splitted_experiment_name) > 2:
            replicate_idx = int(splitted_experiment_name[2])
        else:
            replicate_idx = 1
    else:
        replicate_idx = 0

    # Find the protocol used.
    if 'manual' in protocol_name:
        # Remove the name of the system from the label (include only manual and threshold).
        protocol_name = protocol_name[:len('manual')+2]

    return f'{receptor_name}-ligand{ligand_idx}-{protocol_name}-replicate{replicate_idx}'


def experiment_name_to_info(experiment_name):
    """Extract a few useful info on system, protocol, and replicate number encoded in the experiment name.

    Parameters
    ----------
    experiment_name : str
        The name assigned by get_experiment_name().

    Returns
    -------
    system_name : str
    receptor_name : str
    ligand_idx : int
        For example, for "CB8-ligand3", system_name is "CB8-ligand3",
        receptor_name is "CB8" and ligand_idx is 3.
    protocol_type : str
        Wither "manual" or "trailblaze".
    trailblaze_thermo_length_threshold : float or None
        If manual protocol, this is None.
    replicate_idx : int
        0-based.

    """
    receptor_name, ligand_name, protocol_name, replicate = experiment_name.split('-')
    system_name = receptor_name + '-' + ligand_name
    ligand_idx = int(ligand_name[len('ligand'):])
    if 'manual' in protocol_name:
        protocol_type = 'manual'
    else:
        protocol_type = 'trailblaze'
    trailblaze_thermo_length_threshold = float(protocol_name[len(protocol_type):]) / 10
    replicate_idx = int(replicate[len('replicate'):])
    return (
        system_name, receptor_name, ligand_idx, protocol_type,
        trailblaze_thermo_length_threshold, replicate_idx
    )


def get_remaining_cutoffs(n_energy_eval_interval, cleanup=False, n_iterations=None,
                          exclude=None, read_unmerged=True):
    """Return the number of jobs remaining for each experiment.

    Parameters
    ----------
    n_energy_eval_interval : int, optional
        Number of energy evaluations between analyzed iterations in the
        returned trajectory.
    cleanup : bool, optional, default False
        If True, the temporary files created by parallel jobs will be removed.
    n_iterations : int, optional
        The total number of iterations to consider. If not given, the function
        will read the number of iterations from the netcdf file.
    exclude : Set[str]
        A set of experiment directory paths that are not going to be analyzed.
    read_unmerged : bool, optional, default True
        If False, the recently added files created by analysis jobs are not
        read and they will still appear in the remaining cutoffs.

    Returns
    -------
    remaining_cutoffs : Dict[str, numpy.ndarray]
        A map experiment_directory_path -> n_energy_eval_cutoffs, where
        n_energy_eval_cutoffs is the array of number of energy evaluations
        that still need to be analyzed for that experiment (can be empty).

    """
    # Handle default arguments.
    if exclude is None:
        exclude = set()

    remaining_cutoffs = collections.OrderedDict()

    for experiment_dir_path in sorted(glob.glob(os.path.join(EXPERIMENTS_DIR_PATH, '*', '*'))):
        # Ignore experiments that are marked for exclusion.
        if experiment_dir_path in exclude:
            continue

        experiment_name = get_experiment_name(experiment_dir_path)
        output_file_base_path = os.path.join(
            FREE_ENERGY_TRAJ_DIR_PATH, experiment_name)

        # Check which number of energy evaluations have been already analyzed.
        free_energy_traj = read_free_energy_trajectory(
            output_file_base_path, cleanup=cleanup, read_unmerged=read_unmerged)
        # TODO: REMOVE ME
        if 'replicate0' in experiment_name:
            n_iterations = 10000
        else:
            n_iterations = 5000
        n_energy_eval_cutoffs, _ = get_analysis_cutoffs(experiment_dir_path, n_energy_eval_interval,
                                                        n_iterations=n_iterations)
        cutoffs = np.array([neec for neec in n_energy_eval_cutoffs if neec not in free_energy_traj.data])
        remaining_cutoffs[experiment_dir_path] = cutoffs

    return remaining_cutoffs


def print_remaining_cutoffs(*args, n_cutoffs_per_job=None, **kwargs):
    """Print the number of jobs remaining for each experiment.

    Parameters
    ----------
    n_energy_eval_interval : int
        Number of energy evaluations between analyzed iterations in the
        returned trajectory.
    n_cutoffs_per_job : int, optional
        If given, also the number of jobs is printed beside the number of energy
        evaluations cutoffs.
    cleanup : bool, optional, default False
        If True, the temporary files created by parallel jobs will be removed.
    n_iterations : int, optional
        The total number of iterations to consider. If not given, the function
        will read the number of iterations from the netcdf file.
    exclude : Set[str]
        A set of experiment directory paths that are not going to be analyzed.

    """
    tot_n_jobs = 0
    tot_n_remaining_cutoffs = 0

    remaining_cutoffs = get_remaining_cutoffs(*args, **kwargs)
    for experiment_dir_path, n_energy_eval_cutoffs in remaining_cutoffs.items():
        n_remaining_cutoffs = len(n_energy_eval_cutoffs)
        tot_n_remaining_cutoffs += n_remaining_cutoffs
        if n_cutoffs_per_job is not None:
            n_remaining_jobs = math.ceil(n_remaining_cutoffs / n_cutoffs_per_job)
            tot_n_jobs += n_remaining_jobs
            n_remaining_jobs_str = f'({n_remaining_jobs} jobs)'

        if n_remaining_cutoffs != 0:
            print(experiment_dir_path, ':', n_remaining_cutoffs, n_remaining_jobs_str)

    if tot_n_jobs > 0:
        tot_n_jobs_str = f'({tot_n_jobs} jobs)'
    else:
        tot_n_jobs_str = ''
    print('TOTAL:', tot_n_remaining_cutoffs, tot_n_jobs_str)


def extract_free_energy_trajectories(n_energy_eval_interval, job_id, n_cutoffs_per_job=None, **kwargs):
    """Compute the free energy trajectory.

    The free energies will be separated by the given interval of energy evaluation.

    Parameters
    ----------
    n_energy_eval_interval : int, optional
        Number of energy evaluations between analyzed iterations in the
        returned trajectory.
    job_id : int
        The ID of the job. Jobs are separated by experiments and if n_cutoffs_per_job
        is given each experiment is further split into several jobs.
    n_cutoffs_per_job : int, optional
        If given, the experiment is further split into several jobs that
        can be run in parallel and analyze at most n_cutoffs_per_job energy evaluations.
    n_iterations : int, optional
        The total number of iterations to consider. If not given, the function
        will read the total number of run iterations from the netcdf file.
    exclude : Set[str]
        A set of experiment directory paths that are not going to be analyzed.

    """
    # Get the number of cutoffs that still need to be analyzed.
    # Ignore those that were updated so that the number of jobs
    # doesn't change until we cleanup.
    remaining_cutoffs = get_remaining_cutoffs(n_energy_eval_interval, cleanup=False,
                                              read_unmerged=False, **kwargs)

    # Divide the remaining cutoffs into jobs.
    jobs = []
    for experiment_dir_path, n_energy_eval_cutoffs in remaining_cutoffs.items():
        # Remove experiments that have no number of energy evaluations to analyze.
        if len(n_energy_eval_cutoffs) == 0:
            continue

        if n_cutoffs_per_job is None:
            # One job per experiment.
            jobs.append((experiment_dir_path, n_energy_eval_cutoffs))
        else:
            # Divide the experiment into several jobs of length n_cutoffs_per_job.
            n_jobs = math.ceil(len(n_energy_eval_cutoffs) / n_cutoffs_per_job)
            for i in range(n_jobs):
                jobs.append((experiment_dir_path, n_energy_eval_cutoffs[i::n_jobs]))

    # Analyze the calculation.
    experiment_dir_path, n_energy_eval_cutoffs = jobs[job_id][0], jobs[job_id][1]
    free_energy_traj = get_free_energy_traj(
        experiment_dir_path, n_energy_eval_cutoffs=n_energy_eval_cutoffs)

    # Build path to save the file.
    os.makedirs(FREE_ENERGY_TRAJ_DIR_PATH, exist_ok=True)
    output_file_path = os.path.join(
        FREE_ENERGY_TRAJ_DIR_PATH,
        get_experiment_name(experiment_dir_path) + f'-{job_id}.json'
    )
    with open(output_file_path, 'w') as f:
        json.dump(free_energy_traj, f)


def read_free_energy_trajectory(file_base_path, cleanup=False, read_unmerged=True):
    """Return the free energy trajectory with associated uncertainty of the given experiment.

    Parameter
    ---------
    file_base_path : str
        E.g., "path/to/seeded-trailblaze-2". No extension.
    cleanup : bool, optional, default False
        If True, the temporary files created by parallel jobs will be removed.
    read_unmerged : bool, optional, default True
        If False, the recently added files created by analysis jobs are not
        returned.

    Returns
    -------
    free_energy_traj : FreeEnergyTrajectory
        The free energy trajectory.

    """
    if cleanup and not read_unmerged:
        raise ValueError('Cannot clean up the output of the jobs without reading it.')

    main_file_path = file_base_path + '.json'

    # Load the main file.
    try:
        with open(main_file_path, 'r') as f:
            free_energy_traj = json.load(f)
    except FileNotFoundError:
        free_energy_traj = {}

    # Check the recently added files that need to be merged into one.
    if read_unmerged:
        jobs_output_file_paths = list(glob.glob(file_base_path + '-*.json'))
        for file_path in jobs_output_file_paths:
            with open(file_path, 'r') as f:
                free_energy_traj.update(json.load(f))

    # Update the main file and remove (afterwards) old job output files.
    if cleanup:
        os.makedirs(os.path.dirname(main_file_path), exist_ok=True)
        with open(main_file_path, 'w') as f:
            json.dump(free_energy_traj, f)
        for file_path in jobs_output_file_paths:
            os.remove(file_path)

    # Convert string n_energy_eval keys to integers.
    free_energy_traj = {float(n_energy_eval): data for n_energy_eval, data in free_energy_traj.items()}
    return FreeEnergyTrajectory.from_dict(free_energy_traj)


# ============================================================
# THERMODYNAMIC LENGTH ANALYSIS
# ============================================================

def extract_thermo_length_and_acceptance(experiment_dir_path, as_numpy=True):
    """Estimate the thermo length estimates and the average neighbor acceptance rates.

    Parameters
    ----------
    experiment_dir_path : str
        The path to the experiment directory.
    as_numpy : bool
        If True, the estimates of the thermodynamic length and the average
        acceptance rates will be returned as numpy array, otherwise as a list.

    Returns
    -------
    experiment_data : Dict[str, Dict[str, Iterable]]
        experiment_data[phase_name][estimate] is
    """
    from utils.thermolength import (read_states_energies, estimate_thermo_length_from_definition,
                                    estimate_thermo_length_from_JS, estimate_thermo_length_from_std,
                                    compute_average_neighbor_acceptance_rates)

    # Load the protocol.
    protocol = read_experiment_protocol(experiment_dir_path)

    experiment_data = {}
    for phase_name, phase_protocol in protocol.items():
        phase_data = {}

        # Load MBAR energy matrix.
        nc_file_path = os.path.join(experiment_dir_path, phase_name + '.nc')
        mbar_energies = read_states_energies(nc_file_path)

        # Compute the thermodynamic length estimates
        L_def, dL_def = estimate_thermo_length_from_definition(mbar_energies, phase_protocol)
        phase_data['L_def'] = L_def
        phase_data['dL_def'] = dL_def
        phase_data['L_JS'] = estimate_thermo_length_from_JS(mbar_energies)
        std_thermo_lengths = estimate_thermo_length_from_std(mbar_energies)
        phase_data['L_std_F'] = std_thermo_lengths[0]
        phase_data['L_std_R'] = std_thermo_lengths[1]

        # Compute the neighbor acceptance rates.
        phase_data['acceptance'] = compute_average_neighbor_acceptance_rates(mbar_energies)

        # Convert to list if necessary.
        if not as_numpy:
            for estimate_name, estimate in phase_data.items():
                print(estimate_name, estimate)
                phase_data[estimate_name] = estimate.tolist()

        experiment_data[phase_name] = phase_data

    return experiment_data


def extract_all_thermo_lengths_and_acceptances(job_id=None, dry_run=False):
    """Extract the thermo lengths and acceptance rates."""
    # Find all experiments.
    experiment_dir_paths = sorted(glob.glob(os.path.join(EXPERIMENTS_DIR_PATH, '*', '*')))

    # Check if this is a dry run.
    if dry_run:
        jobs_to_run = []

    # Find the experiment associated to this job ID. Otherwise, run all.
    if not dry_run and job_id is not None:
        # Job IDs are 1-based so we convert it to an index.
        experiment_dir_paths = [experiment_dir_paths[job_id-1]]

    for exp_idx, experiment_dir_path in enumerate(experiment_dir_paths):
        # Determine output path file.
        os.makedirs(THERMO_LEN_DIR_PATH, exist_ok=True)
        output_file_path = os.path.join(
            THERMO_LEN_DIR_PATH, get_experiment_name(experiment_dir_path) + '.json')

        # Check if this was already computed.
        if os.path.isfile(output_file_path):
            continue

        # Otherwise, flag this for running or compute the estimates.
        if dry_run:
            print('job_id =', exp_idx+1, ':', experiment_dir_path)
            jobs_to_run.append(exp_idx+1)
        else:
            # Compute the estimates and store it.
            experiment_data = extract_thermo_length_and_acceptance(experiment_dir_path, as_numpy=False)
            with open(output_file_path, 'w') as f:
                json.dump(experiment_data, f)

    # Plot all the jobs to run.
    if dry_run:
        from yank.commands.status import find_contiguous_ids
        print('Jobs to run:', find_contiguous_ids(jobs_to_run))


# ============================================================
# MAIN
# ============================================================

if __name__ == '__main__':

    # import logging
    # logging.basicConfig(
    #     level=logging.DEBUG,
    #     format='%(asctime)-15s: %(levelname)s - %(name)s - %(message)s'
    # )

    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--jobid', type=int)
    args = parser.parse_args()

    n_energy_eval_interval = 1e6
    n_cutoffs_per_job = 10
    exclude = {
        '../yank/experiments/experiment-CB8-main/trailblaze05_CB8systemCB8ligands1',
        '../yank/experiments/experiment-T4-main/trailblaze05_T4systemT4L99Aligands14',
        '../yank/experiments/experiment-T4-main/trailblaze05_T4systemT4L99Aligands4',
        '../yank/experiments/experiment-T4-main/trailblaze05_T4systemT4L99Aligands9'
    }

    # DO NOT RUN THIS IN PARALLEL WITH cleanup=True IF THERE ARE TEMPORARY
    # JOB-ID RESULTS TO MERGE OR YOU'LL RISK TO LOSE DATA!
    # print_remaining_cutoffs(n_energy_eval_interval, cleanup=True, read_unmerged=True, n_cutoffs_per_job=n_cutoffs_per_job, exclude=exclude)
    # extract_free_energy_trajectories(n_energy_eval_interval, args.jobid-1, n_cutoffs_per_job=n_cutoffs_per_job, exclude=exclude)

    # Extract thermo length and neighbor acceptance rates.
    extract_all_thermo_lengths_and_acceptances(job_id=args.jobid, dry_run=False)
